---
title: "Blocking: An R Package for Blocking of Records for Record Linkage and Deduplication"
date: "2025-11-09"
abstract: >
  Entity resolution (probabilistic record linkage, deduplication) is essential for estimation based on multiple sources. It aims to link records without common identifiers that refer to the same entity (e.g., person, company). Without identifiers, researchers must specify which records to compare to calculate matching probability and reduce computational complexity. Traditional deterministic blocking uses common variables like names or dates of birth, but assumes error-free, complete data. To address this limitation, we developed the R package \CRANpkg{blocking}, which uses approximate nearest neighbour search and graph algorithms to reduce comparisons. This paper presents the package design, functionalities, and two official statistics case studies.
draft: true
author:  
  - name: Maciej Beręsewicz
    affiliation: 
      - University of Economics and Business
      - Statisical Office in Poznań
    address:
      - Department of Statistics, Poznań, Poland
      - Centre for the Methodology of Population Studies
    url: https://maciejberesewicz.com
    orcid: 0000-0002-8281-4301
    email:  maciej.beresewicz@poznan.pl
  - name: Adam Struzik
    affiliation:
      - Adam Mickiewicz University
      - Statisical Office in Poznań
    address:
      - Department of Mathematics, Poznań, Poland
      - Centre for Urban Statistics
    email: adastr5@st.amu.edu.pl
type: package
output: 
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
bibliography: RJreferences.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
library(plotly)
library(ggplot2)
library(palmerpenguins)
library(kableExtra)
```

# Introduction

This paper presents the \CRANpkg{blocking} package that aims to make the linkage and deduplication easier by creating the..

The following should be underlined:

+ reduce number of comparisons
+ reduce FNR and other errors
+ reduce workload on clerical review


# Background

Some packages on interactive graphics include \CRANpkg{plotly} [@plotly] that interfaces with Javascript for web-based interactive graphics, \CRANpkg{crosstalk} [@crosstalk] that specializes cross-linking elements across individual graphics. The recent R Journal paper \CRANpkg{tsibbletalk} [@RJ-2021-050] provides a good example of including interactive graphics into an article for the journal. It has both a set of linked plots, and also an animated gif example, illustrating linking between time series plots and feature summaries.

# Blocking of records using `blocking` function

## The main function

## Assessment of results

In the package we have implemented several measures that can be used to assess the results

**Reduction Ratio**: Provides necessary details about the reduction in comparison pairs if the given blocks are applied to a further record linkage or deduplication procedure. For deduplication:

$$
\text{RR}_{\text{deduplication}} = 1 - \frac{\sum\limits_{i=1}^{k} \binom{|B_i|}{2}}{\binom{n}{2}},
$$

where $k$ is the total number of blocks, $n$ is the total number of records in the dataset, and $|B_i|$ is the number of records in the $i$-th block. $\sum\limits_{i=1}^{k} \binom{|B_i|}{2}$ is the number of comparisons after blocking, while $\binom{n}{2}$ is the total number of possible comparisons without blocking. For record linkage the reduction ratio is defined as follows

$$
\text{RR}_{\text{record\_linkage}} = 1 - \frac{\sum\limits_{i=1}^{k} |B_{i,x}| \cdot |B_{i,y}|} {(m \cdot n)},
$$

where $m$ and $n$ are the sizes of datasets $X$ and $Y$, and $k$ is the total number of blocks. The term $|B_{i,x}|$ is the number of unique records from dataset $X$ in the $i$-th block, while $|B_{i,y}|$ is the number of unique records from dataset $Y$ in the $i$-th block. The expression $\sum\limits_{i=1}^{k} |B_{i,x}| \cdot |B_{i,y}|$ is the number of comparisons after blocking.

Confusion matrix presents results in comparison to ground-truth \texttt{blocks} in a pairwise manner (e.g., one true positive pair occurs when both records from the comparison pair belong to the same predicted \texttt{block} and to the same ground-truth \texttt{block} in the evaluation data frame). 

+ True Positive (TP): Record pairs correctly matched in the same block.
+ False Positive (FP): Records pairs identified as matches that are not true matches in the same block.
+ True Negative (TN): Record pairs correctly identified as non-matches (different blocks)
+ False Negative (FN): Records identified as non-matches that are true matches in the same block.

| **Metric** | **Formula** | **Metric** | **Formula** |
|------------|-------------|------------|-------------|
| Recall | $\frac{TP}{TP + FN}$ | Accuracy | $\frac{TP + TN}{TP + TN + FP + FN}$ |
| Precision | $\frac{TP}{TP + FP}$ | Specificity | $\frac{TN}{TN + FP}$ |
| F1 Score | $2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ | False Positive Rate | $\frac{FP}{FP + TN}$ |
| False Negative Rate | $\frac{FN}{FN + TP}$ | | |

*Table: Evaluation Metrics*

# Case studies

## Record linkage example

Let us first load the required packages.

```{r packages, echo=TRUE, message=FALSE, warning=FALSE}
library(blocking)
library(data.table)
```

We demonstrate the use of `blocking` function for record linkage on the `foreigners` dataset included in the package. This fictional representation of the foreign population in Poland was generated based on publicly available information, preserving the distributions from administrative registers. It contains 110,000 rows with 100,000 entities. Each row represents one record, with the following columns:

- `fname` -- first name,
- `sname` -- second name,
- `surname` -- surname,
- `date` -- date of birth,
- `region` -- region (county),
- `country` -- country,
- `true_id` -- person ID.

```{r foreigners, echo = TRUE}
data(foreigners)
head(foreigners)
```

We split the dataset into two separate files: one containing the first appearance of each entity in the `foreigners` dataset, and the other containing its subsequent appearances.

```{r split, echo = TRUE}
foreigners_1 <- foreigners[!duplicated(foreigners$true_id), ]
foreigners_2 <- foreigners[duplicated(foreigners$true_id), ]
```

Now in both datasets we remove slashes from the `date` column and create a new string column that concatenates the information from all columns (excluding `true_id`) in each row.

```{r concat, echo = TRUE}
foreigners_1[, date := gsub("/", "", date)]
foreigners_1[, txt := paste0(fname, sname, surname, date, region, country)]
foreigners_2[, date := gsub("/", "", date)]
foreigners_2[, txt := paste0(fname, sname, surname, date, region, country)]
head(foreigners_1)
```

### General use

We use the newly created columns in the `blocking` function, which relies on the default \CRANpkg{rnndescent} (Nearest Neighbor Descent) algorithm based on cosine distance. Additionally, we set `verbose = 1` to monitor progress. Note that a default parameter of the `blocking` function is `seed = 2023`, which sets the random seed.

```{r reclin_nnd, echo = TRUE}
result_reclin <- blocking(x = foreigners_1$txt,
                          y = foreigners_2$txt,
                          verbose = 1)
```

```{r reclin_nnd_calcs}
blocks_tab <- table(result_reclin$result$block)
block_ids <- rep(as.numeric(names(blocks_tab)), blocks_tab+1)
block_size <- as.numeric(names(table(table(block_ids))))
block_count <- as.vector(table(table(block_ids)))
```

Now we examine the results of record linkage.

- We have created `r format(NROW(unique(result_reclin$result$block)), big.mark = ",")` blocks.
- The blocking process utilized `r format(NROW(result_reclin$colnames), big.mark = ",")` columns (2 character shingles).
- We have `r format(block_count[1], big.mark = ",")` blocks of `r block_size[1]` elements, `r format(block_count[2], big.mark = ",")` blocks of `r block_size[2]` elements,..., `r format(block_count[NROW(block_count)], big.mark = ",")` blocks of `r block_size[NROW(block_size)]` elements.

```{r reclin_nnd_summary, echo = TRUE}
result_reclin
```

Structure of the object is as follows:

- `result` -- a `data.table` with identifiers and block IDs,
- `method` -- name of the ANN algorithm used,
- `deduplication` -- whether deduplication was applied,
- `representation` -- whether shingles or vectors were used,
- `metrics` -- metrics for quality assessment (here `NULL`),
- `confusion` -- confusion matrix (here `NULL`),
- `colnames` -- column names used for the comparison,
- `graph` -- an \CRANpkg{igraph} object, mainly for visualization (here `NULL`).

```{r reclin_nnd_str, echo = TRUE}
str(result_reclin, 1)
```

The resulting `data.table` has four columns:

- `x` -- reference dataset (i.e. `foreigners_1`) -- this may not contain all units of `foreigners_1`,
- `y` -- query (each row of `foreigners_2`) -- this may not contain all units of `foreigners_2`,
- `block` -- block ID,
- `dist` -- distance between objects.

```{r reclin_nnd_result, echo = TRUE}
head(result_reclin$result)
```

Let's examine the first pair. Obviously, there are typos in the `fname` and `surname`. Nevertheless, the pair is a match.

```{r reclin_nnd_example, echo = TRUE}
cbind(t(foreigners_1[3, 1:6]), t(foreigners_2[1, 1:6]))
```

Now we use the `true_id` values to evaluate our approach.

```{r reclin_nnd_matches, echo = TRUE}
matches <- merge(x = foreigners_1[, .(x = 1:.N, true_id)],
                 y = foreigners_2[, .(y = 1:.N, true_id)],
                 by = "true_id")
matches[, block := rleid(x)]
head(matches)
```

We have 10,000 matched pairs. We use the `true_blocks` parameter in the `blocking` function to specify the true block assignments. We obtain the quality metrics for the assessment of record linkage.

```{r reclin_nnd_true_blocks, echo = TRUE}
result_2_reclin <- blocking(x = foreigners_1$txt,
                            y = foreigners_2$txt,
                            verbose = 1,
                            true_blocks = matches[, .(x, y, block)])
result_2_reclin
```

For example, our approach results in a `r sprintf("%.2f", (result_2_reclin$metrics)[4]*100)`% false negative rate (FNR). To improve this, we can increase the `epsilon` parameter of the NND method from 0.1 to 0.5. To do so, we configure the `control_ann` parameter in the `blocking` function using the `controls_ann` and `control_nnd` functions.

```{r reclin_nnd_improved, echo = TRUE}
result_3_reclin <- blocking(x = foreigners_1$txt,
                            y = foreigners_2$txt,
                            verbose = 1,
                            true_blocks = matches[, .(x, y, block)],
                            control_ann = controls_ann(nnd = control_nnd(epsilon = 0.5)))
result_3_reclin
```

That decreases the FNR to `r sprintf("%.2f", (result_3_reclin$metrics)[4]*100)`%.

## Deduplication example

We demonstrate deduplication using the `blocking` function on the `RLdata500` dataset from the \CRANpkg{RecordLinkage} package. Note that the dataset is included in the `blocking` package. It contains artificial personal data. Fifty records have been duplicated with randomly generated errors. Each row represents one record, with the following columns:

- `fname_c1` -- first name, first component,
- `fname_c2` -- first name, second component,
- `lname_c1` -- last name, first component,
- `lname_c2` -- last name, second component,
- `by` -- year of birth,
- `bm` -- month of birth,
- `bd` -- day of birth,
- `rec_id` -- record id,
- `ent_id` -- entity id.

```{r RLdata500, echo = TRUE}
data(RLdata500)
head(RLdata500)
```

We create a new column (`id_count`) that indicates how many times a given unit occurs and then add leading zeros to the `bm` and `bd` columns. Finally, we create a new string column that concatenates the information from all columns (excluding `rec_id`, `ent_id` and `id_count`) in each row.

```{r RLdata500_concat, echo = TRUE}
RLdata500[, id_count :=.N, ent_id]
RLdata500[, bm:=sprintf("%02d", bm)]
RLdata500[, bd:=sprintf("%02d", bd)]
RLdata500[, txt:=tolower(paste0(fname_c1,fname_c2,lname_c1,lname_c2,by,bm,bd))]
head(RLdata500)
```

As in the previous example, we use the `txt` column in the `blocking` function. This time, we set `ann = hnsw` to use the Hierarchical Navigable Small World (HNSW) algorithm from the \CRANpkg{RcppHNSW} package and `graph = TRUE` to obtain an \CRANpkg{igraph} object for visualization.

```{r dedup_hnsw, echo = TRUE}
result_dedup_hnsw <- blocking(x = RLdata500$txt,
                              ann = "hnsw",
                              graph = TRUE,
                              verbose = 1)
```

The results are as follows.

```{r dedup_hnsw_result, echo = TRUE}
result_dedup_hnsw
head(result_dedup_hnsw$result)
```

Now we visualize connections using the obtained graph.

```{r dedup_graph, echo = TRUE, out.width = "100%", fig.width = 6, fig.height=5, layout="l-body"}
plot(result_dedup_hnsw$graph, vertex.size = 1, vertex.label = NA)
```

We create a long `data.table` with information on blocks and units from the original dataset.

```{r dedup_melted, echo = TRUE}
df_block_melted <- melt(result_dedup_hnsw$result, id.vars = c("block", "dist"))
df_block_melted_rec_block <- unique(df_block_melted[, .(rec_id=value, block)])
head(df_block_melted_rec_block)
```

We add the block information to the final dataset.

```{r dedup_blocks, echo = TRUE}
RLdata500[df_block_melted_rec_block, on = "rec_id", block_id := i.block]
head(RLdata500)
```

We can check in how many blocks the same entities (`ent_id`) are observed. In our example, all the same entities are in the same blocks.

```{r dedup_uniq_blocs, echo = TRUE}
RLdata500[, .(uniq_blocks = uniqueN(block_id)), .(ent_id)][, .N, uniq_blocks]
```

Now we can visualize the distances between the units stored in the `result_dedup_hnsw$result` dataset. Clearly we have a mixture of two groups: matches (close to 0) and non-matches (close to 1).

```{r dedup_hist, echo = TRUE, out.width = "100%", fig.width = 6, fig.height=5, layout="l-body"}
hist(result_dedup_hnsw$result$dist, xlab = "Distances", ylab = "Frequency", breaks = "fd",
     main = "Distances calculated between units")
```

Finally, we visualize the result based on the information whether a block contains matches or not.

```{r dedup_density, echo = TRUE, out.width = "100%", fig.width = 6, fig.height=5, layout="l-body"}
df_for_density <- copy(df_block_melted[block %in% RLdata500$block_id])
df_for_density[, match:= block %in% RLdata500[id_count == 2]$block_id]

plot(density(df_for_density[match==FALSE]$dist), col = "blue", xlim = c(0, 0.8), 
     main = "Distribution of distances between\nclusters type (match=red, non-match=blue)")
lines(density(df_for_density[match==TRUE]$dist), col = "red", xlim = c(0, 0.8))
```

Now we compare the evaluation metrics across all ANN algorithms supported by the `blocking` function, i.e. NND, HNSW, Approximate Nearest Neighbors Oh Yeah (Annoy, from the \CRANpkg{RcppAnnoy} package), Locality-sensitive hashing (LSH, from the \CRANpkg{mlpack} package), and k-Nearest Neighbors (kNN -- denoted as `"kd"`, from the \CRANpkg{mlpack} package). We use the `rec_id` and `ent_id` columns from the `RLdata500` dataset to specify the true blocks and then calculate evaluation metrics for all algorithms. Additionally, we assess blocking using the `klsh` function from the \CRANpkg{klsh} package, configured to create 10 blocks and 100 blocks, respectively. In both settings, we use 20 random projections and 2-character shingles. The results are as follows (`klsh_10` and `klsh_100` refer to the `klsh` algorithm with 10 blocks and 100 blocks, respectively).

```{r comparision, echo = TRUE}
true_blocks <- RLdata500[, c("rec_id", "ent_id"), with = FALSE]
setnames(true_blocks, old = c("rec_id", "ent_id"), c("x", "block"))
eval_metrics <- list()
ann <- c("nnd", "hnsw", "annoy", "lsh","kd")
for (algorithm in ann) {
  eval_metrics[[algorithm]] <- blocking(x = RLdata500$txt,
                                ann = algorithm,
                                true_blocks = true_blocks)$metrics
}

set.seed(2025)
blocks_klsh_10 <- klsh::klsh(r.set = RLdata500[, c("fname_c1", "fname_c2", "lname_c1",
                                                   "lname_c2", "by", "bm", "bd")],
                             p = 20,
                             num.blocks = 10,
                             k = 2)
klsh_10_metrics <- klsh::confusion.from.blocking(blocking = blocks_klsh_10, 
                                                 true_ids = RLdata500$ent_id)[-1]
klsh_10_metrics$f1_score <- 2 * klsh_10_metrics$precision * klsh_10_metrics$recall / 
  (klsh_10_metrics$precision + klsh_10_metrics$recall)
eval_metrics$klsh_10 <- unlist(klsh_10_metrics)
blocks_klsh_100 <- klsh::klsh(r.set = RLdata500[, c("fname_c1", "fname_c2", "lname_c1",
                                                    "lname_c2", "by", "bm", "bd")],
                              p = 20,
                              num.blocks = 100,
                              k = 2)
klsh_100_metrics <- klsh::confusion.from.blocking(blocking = blocks_klsh_100, 
                                                 true_ids = RLdata500$ent_id)[-1]
klsh_100_metrics$f1_score <- 2 * klsh_100_metrics$precision * klsh_100_metrics$recall /
  (klsh_100_metrics$precision + klsh_100_metrics$recall)
eval_metrics$klsh_100 <- unlist(klsh_100_metrics)

do.call(rbind, eval_metrics) * 100
```


# Summary

In this paper we have demonstrated the basic use cases of the \CRANpkg{blocking} package. We believe that the software will be useful for researchers working in various fields where integration of multiple sources is an important aspect. 

# Acknowledgements

Work on this package is supported by the National Science Centre, OPUS 20 grant no. 2020/39/B/HS4/00941. We also thank participants of the uRos 2024 conference for valuable comments and discussion. 

We also have developed a python version of the package {BlockingPy} that is available through the PiPy. It has the similar structure but offers more ANN algorithms (e.g. FAISS) or usage of embeddings. For more details see: Strojny, T., & Beręsewicz, M. (2025). BlockingPy: approximate nearest neighbours for blocking of records for entity resolution. arXiv preprint arXiv:2504.04266.

