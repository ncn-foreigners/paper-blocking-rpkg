---
title: "blocking: An R Package for Blocking of Records for Record Linkage and Deduplication"
date: "2025-11-09"
abstract: >
  Entity resolution (probabilistic record linkage, deduplication) is essential for estimation based on multiple sources. It aims to link records without common identifiers that refer to the same entity (e.g., person, company). Without identifiers, researchers must specify which records to compare to calculate matching probability and reduce computational complexity. Traditional deterministic blocking uses common variables like names or dates of birth, but assumes error-free, complete data. To address this limitation, we developed the R package \CRANpkg{blocking}, which uses approximate nearest neighbour search and graph algorithms to reduce number of  comparisons. This paper presents the package design, functionalities, and two case studies.
draft: true
author:  
  - name: Maciej Beręsewicz
    affiliation: 
      - University of Economics and Business
      - Statisical Office in Poznań
    address:
      - Department of Statistics, Poznań, Poland
      - Centre for the Methodology of Population Studies
    url: https://maciejberesewicz.com
    orcid: 0000-0002-8281-4301
    email: maciej.beresewicz@poznan.pl
  - name: Adam Struzik
    affiliation:
      - Adam Mickiewicz University
      - Statisical Office in Poznań
    address:
      - Department of Mathematics, Poznań, Poland
      - Centre for Urban Statistics
    email: adastr5@st.amu.edu.pl
type: package
output: 
  rjtools::rjournal_pdf_article:
    toc: no
  rjtools::rjournal_web_article:
    self_contained: yes
    toc: no
header-includes:
  - \usepackage{float}
bibliography: RJreferences.bib
execute:
  cache: true
---

# Introduction

## Blocking for record linkage

Entity resolution (probabilistic record linkage, deduplication) is essential for estimation based on multiple sources (for recent review see @Binette2022). The goal is to link records without common identifiers that refer to the same entity (e.g., person, company). This situation is often observed in administrative records, in particular for foreign-born populations. For instance, the Social Insurance Institution register in Poland at the end of 2023 included 1.206 million records which referred to possibly 1.105 million individuals, out of which about 10% had missing information in the personal identifier (PESEL) and about 50% of cases had missing address details. Please note that the exact number of individuals will be certainly lower than 1.105 million as the 10% may include duplicates. <!---- add citation to the conference here --->

This drives a need to link records without identifiers but often requires certain assumptions such as how to reduce the large number of possible comparisons as it is not possible to compare all pairs of records in a large dataset (e.g., for the mentioned example this would lead to over 600 billion comparisons). That is why *blocking* methods are applied to reduce the number of comparisons prior to the final record linkage/deduplication stage not only because of computational reasons but also due to clerical review workload.

Blocking is a method of reducing the number of possible comparisons by assuming that certain variables should be exactly matched. For instance, a standard method is based on assuming that sex or age (or some other combination) should match exactly while other characteristics of the records could be varying. Another standard method is to use phonetic algorithms such as SOUNDEX or its improvements for non-English languages. Furthermore, due to the use of large language models one may also consider using embeddings to search for the closest neighbor and treat this as a possible pair. For a review of blocking methods see @Steorts2014 or @Papadakis2020 and in Section \@ref(sec-software) we will discuss R packages that implement blocking methods.

Reducing the number of pairs has its costs: missing comparisons which lead to an increased false positive rate (FPR) and false negative rate (FNR) of the linkage study. In order to assess this error, a subset of pairs or simulation studies should be applied. Alternatively, one may consider approaches proposed by @dasylva2021estimating and @dasylva2022consistent who proposed methods to estimate FPR and FNR without access to an audit sample.

## Existing software and our contribution {#sec-software}

The R system offers several packages that implements various blocking techniques which we grouped by the following classification:

+ deterministic blocking: 
    + \CRANpkg{reclin2} [@reclin2, @reclin2-rjournal] which allows to pair records using the `pair_blocking()` with a prespecified list of columns in a `data.frame`, and the `pair_minsim()` function that allows to specify the minimal similarity score (e.g. 1 out of 3 variables should match exactly).
    + \CRANpkg{RecordLinkage} [@RecordLinkage, @RecordLinkage-rjournal] which allows to specify blocking variable in the `blockfld` in either in `compare.dedup()` or `compare.linkage()` functions in a form of a vector (either character or numeric).
    + \CRANpkg{fastLink} [@fastLink, @enamorado2019using] which implements various blocking methods via the `blockData()` function such as exact matching, window matching (e.g., no more than 2 years difference between birth year) or k-means clustering algorithm. It should be noted that the `fastLink` returns splits dataset(s) into a separate lists while `reclin2` and `RecordLinkage` package create a single dataset.

+ phonetic blocking: 
    + \CRANpkg{RecordLinkage} allows to directly specify the phonetic comparison via the `phonetic` argument of the `compare.dedup()` or `compare.linkage()` function via the `soundex()` function. However, this is not used for blocking but for comparison of strings
    + It should be noted that \CRANpkg{stringdist} [@stringdist] also implements SOUNDEX algorithm while the \CRANpkg{phonics} [@phonics, @Phonetic2020] implements various phonetic algorithms that could be applied prior the blocking procedure (e.g., create a new column). 
    
+ probabilistic blocking:
    + \CRANpkg{klsh} [@klsh] is the only R package that implements probabilistic blocking using the k-means variant of locality sensitive hashing. The main `klsh()` function implements this approach and a resulting object is a list with row identifiers along for the pre specified number of blocks (via the `num.blocks` argument of the  `klsh()` function).

Unfortunately, practice is more complicated as missing data can be present in important variables (such as birth date) or typos in the names and surnames. That is why we decided to develop \CRANpkg{blocking} that leverage approximate nearest neighbours (ANN) algorithms and graphs to create a large number of small blocks that can be further used in the analysis. The basic idea behind the package can be expressed in the following steps:

1. create shingles via the \CRANpkg{tokenizers} [@tokenizers] and \CRANpkg{text2vec} [@text2vec] packages or a matrix of vectors (e.g. embeddings via the \CRANpkg{ragnar} [@ragnar] package).
2. search for nearest neighbours using approximate algorithms implemented in the \CRANpkg{rnndescent} [@rnndescent], \CRANpkg{RcppHNSW} [@RcppHNSW], \CRANpkg{mlpack} [@mlpack2023, @mlpack2025], and \CRANpkg{RcppAnnoy} [@RcppAnnoy].
3. create blocks using \CRANpkg{igraph} [@igraph2025, @igraph2006].

This is the solely package in the R ecosystem that allows to easily apply modern ANN algorithms and significantly speed-up the record linkage / deduplication problems. In addition, we have developed the `pair_ann()` function to seamless integrate with the \CRANpkg{reclin2} package.

## Outline of article

The paper has the following structure. In the Section \@ref(sec-blocks) we provide description of the main functionalities of the `blocking` package and how we can assess the result. In the Section \@ref(sec-case) we provide two case studies: probabilistic record linkage and deduplication. These examples show how our package can improve pipeline of entity resolution and work with existing R packages.

# Blocking of records using `blocking` function {#sec-blocks}

## The main function

The main functionality is available via the `blocking()` function which contains the following main arguments:

+ `x, y` -- reference datasets, where `y = NULL` which indicate that the deduplication is applied,
+ `representation` -- whether `x` and `y` should be represented as shingles or vectors (provided by the user in the `model` argument),
+ `ann` -- what ANN algorithm should be applied, by default we use the \CRANpkg{rnndescent} package as it allows supports sparse matrices,
+ `distance` -- what should be applied (default is `cosine` distance),
+ `graph` -- whether the plot of the graph of connected records should be returned (default `FALSE`),
+ `true_blocks` -- if a subset of true blocks is available it can be provided here so we measures of quality presented in Section \@ref(sec-assess) are returned,
+ `n_threads` -- how many threads are applied for computation,
+ `control_txt` -- controls provided in the `controls_txt()` on how the `x, y` are processed,
+ `control_ann` -- controls provided in the `controls_ann()` allows user to fine-tune ANN algorithm (see documentation of the `controls_ann()` function and `control_*` functions with the names referring to a specific algorithm, e.g., `control_nnd()` for the NND algorithm).

This function return an object of `blocking` class with the following elements:

+ `result` -- data.table with indices (rows) of x, y, block and distance between points
+ `method` -- name of the ANN algorithm used,
+ `deduplication` -- information whether deduplication was applied,
+ `representation` -- information whether shingles or vectors were used,
+ `metrics` -- metrics for quality assessment, if true_blocks is provided,
+ `confusion` -- confusion matrix, if `true_blocks` is provided,
+ `colnames` -- variable names (`colnames`) used for search,
+ `graph` -- igraph class object.

## Assessment of results {#sec-assess}

In the package we have implemented several measures that can be used to
assess the results

**Reduction Ratio**: Provides necessary details about the reduction in
comparison pairs if the given blocks are applied to a further record
linkage or deduplication procedure. For deduplication:

$$
\text{RR}_{\text{deduplication}} = 1 - \frac{\sum\limits_{i=1}^{k} \binom{|B_i|}{2}}{\binom{n}{2}},
$$

where $k$ is the total number of blocks, $n$ is the total number of
records in the dataset, and $|B_i|$ is the number of records in the
$i$-th block. $\sum\limits_{i=1}^{k} \binom{|B_i|}{2}$ is the number of
comparisons after blocking, while $\binom{n}{2}$ is the total number of
possible comparisons without blocking. For record linkage the reduction
ratio is defined as follows

$$
\text{RR}_{\text{record\_linkage}} = 1 - \frac{\sum\limits_{i=1}^{k} |B_{i,x}| \cdot |B_{i,y}|} {(m \cdot n)},
$$

where $m$ and $n$ are the sizes of datasets $X$ and $Y$, and $k$ is the
total number of blocks. The term $|B_{i,x}|$ is the number of unique
records from dataset $X$ in the $i$-th block, while $|B_{i,y}|$ is the
number of unique records from dataset $Y$ in the $i$-th block. The
expression $\sum\limits_{i=1}^{k} |B_{i,x}| \cdot |B_{i,y}|$ is the
number of comparisons after blocking.

Confusion matrix presents results in comparison to ground-truth
\texttt{blocks} in a pairwise manner (e.g., one true positive pair
occurs when both records from the comparison pair belong to the same
predicted \texttt{block} and to the same ground-truth \texttt{block} in
the evaluation data frame).

-   True Positive (TP): Record pairs correctly matched in the same
    block.
-   False Positive (FP): Records pairs identified as matches that are
    not true matches in the same block.
-   True Negative (TN): Record pairs correctly identified as non-matches
    (different blocks)
-   False Negative (FN): Records identified as non-matches that are true
    matches in the same block.

| **Metric** | **Formula** | **Metric** | **Formula** |
|------------------|------------------|------------------|------------------|
| Recall | $\frac{TP}{TP + FN}$ | Accuracy | $\frac{TP + TN}{TP + TN + FP + FN}$ |
| Precision | $\frac{TP}{TP + FP}$ | Specificity | $\frac{TN}{TN + FP}$ |
| F1 Score | $2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ | False Positive Rate | $\frac{FP}{FP + TN}$ |
| False Negative Rate | $\frac{FN}{FN + TP}$ |  |  |

*Table: Evaluation Metrics*

# Case studies {#sec-case}

## Record linkage example

Let us first load the required packages.

```{r packages, echo=TRUE, message=FALSE, warning=FALSE}
library("blocking")
library("data.table")
```

We demonstrate the use of `blocking` function for record linkage on the
`foreigners` dataset included in the package. This fictional
representation of the foreign population in Poland was generated based
on publicly available information, preserving the distributions from
administrative registers. It contains 110,000 rows with 100,000
entities. Each row represents one record, with the following columns:

-   `fname` -- first name,
-   `sname` -- second name,
-   `surname` -- surname,
-   `date` -- date of birth,
-   `region` -- region (county),
-   `country` -- country,
-   `true_id` -- person ID.

Next, we load the data 

```{r foreigners, echo = TRUE}
data("foreigners")
head(foreigners)
```

We split the dataset into two separate files: one containing the first
appearance of each entity in the `foreigners` dataset, and the other
containing its subsequent appearances.

```{r split, echo = TRUE}
foreigners_1 <- foreigners[!duplicated(foreigners$true_id), ]
foreigners_2 <- foreigners[duplicated(foreigners$true_id), ]
```

Now in both datasets we remove separators in the `date` column and create
a new character column that concatenates the information from all columns
(excluding `true_id`) in each row.

```{r concat, echo = TRUE}
foreigners_1[, date := gsub("/", "", date)]
foreigners_1[, txt := paste0(fname, sname, surname, date, region, country)]
foreigners_2[, date := gsub("/", "", date)]
foreigners_2[, txt := paste0(fname, sname, surname, date, region, country)]
head(foreigners_1)
```

### General use

We use the newly created columns in the `blocking` function, which
relies on the default \CRANpkg{rnndescent} algorithm based on the cosine distance. 
Additionally, we set `verbose = 1`
to monitor progress. Note that a default parameter of the `blocking`
function is `seed = 2023`, which sets the random seed.

```{r reclin_nnd, echo = TRUE}
result_reclin <- blocking(x = foreigners_1$txt,
                          y = foreigners_2$txt,
                          verbose = 1)
```

```{r reclin_nnd_calcs}
blocks_tab <- table(result_reclin$result$block)
block_ids <- rep(as.numeric(names(blocks_tab)), blocks_tab+1)
block_size <- as.numeric(names(table(table(block_ids))))
block_count <- as.vector(table(table(block_ids)))
```

Now we examine the results of record linkage by printing the `result_reclin` object. We have created
`r format(NROW(unique(result_reclin$result$block)), big.mark = ",")` blocks based on
`r format(NROW(result_reclin$colnames), big.mark = ",")` columns (2 character shingles). Blocks are small as we e have `r format(block_count[1], big.mark = ",")` blocks of `r block_size[1]` elements, 
`r format(block_count[2], big.mark = ",")` blocks of `r block_size[2]` elements,..., 
`r format(block_count[NROW(block_count)], big.mark = ",")` blocks of `r block_size[NROW(block_size)]` elements.

```{r reclin_nnd_summary, echo = TRUE}
result_reclin
```

In order to access the result one should use `result_reclin$result`. The resulting `data.table` has four columns (as presented below):

-   `x` -- reference dataset (i.e. `foreigners_1`) -- this may not
    contain all units of `foreigners_1`,
-   `y` -- query (each row of `foreigners_2`) -- this will contain all units of `foreigners_2`,
-   `block` -- the block ID,
-   `dist` -- distance between objects.

```{r reclin_nnd_result, echo = TRUE}
head(result_reclin$result)
```


Let's examine the first pair. Obviously, there are typos in the `fname`
and `surname`. Nevertheless, the pair is a match.

```{r reclin_nnd_example, echo = TRUE}
cbind(t(foreigners_1[3, 1:6]), t(foreigners_2[1, 1:6]))
```

Now we use the `true_id` values to evaluate our approach.

```{r reclin_nnd_matches, echo = TRUE}
matches <- merge(x = foreigners_1[, .(x = 1:.N, true_id)],
                 y = foreigners_2[, .(y = 1:.N, true_id)],
                 by = "true_id")
matches[, block := rleid(x)]
head(matches)
```

We have 10,000 matched pairs which can be used in the `true_blocks` argument in the
`blocking()` function to specify the true block assignments. We obtain the
quality metrics for the assessment of record linkage.

```{r reclin_nnd_true_blocks, echo = TRUE}
result_2_reclin <- blocking(x = foreigners_1$txt,
                            y = foreigners_2$txt,
                            verbose = 1,
                            true_blocks = matches[, .(x, y, block)])
result_2_reclin
```

For example, our approach results in a
`r sprintf("%.2f", (result_2_reclin$metrics)[4]*100)`% false negative
rate (FNR). To improve this, we can increase the `epsilon` parameter of
the NND method from 0.1 to 0.5. To do so, we configure the `control_ann`
parameter in the `blocking` function using the `controls_ann` and
`control_nnd` functions.

```{r reclin_nnd_improved, echo = TRUE}
result_3_reclin <- blocking(x = foreigners_1$txt,
                            y = foreigners_2$txt,
                            verbose = 1,
                            true_blocks = matches[, .(x, y, block)],
                            control_ann = controls_ann(nnd = control_nnd(epsilon = 0.5)))
result_3_reclin
```

That decreases the FNR to
`r sprintf("%.2f", (result_3_reclin$metrics)[4]*100)`%.

## Deduplication example

Next, we demonstrate deduplication using the `blocking` function on the
`RLdata500` dataset from the \CRANpkg{RecordLinkage} package. Note that
the dataset is included in the `blocking` package. It contains
artificial personal data and fifty records have been duplicated with
randomly generated errors. Each row represents one record, with the
following columns:

-   `fname_c1` -- first name, first component,
-   `fname_c2` -- first name, second component,
-   `lname_c1` -- last name, first component,
-   `lname_c2` -- last name, second component,
-   `by`, `bm`, `bd` -- year, month and day of birth,
-   `rec_id` -- record id,
-   `ent_id` -- entity id.

```{r RLdata500, echo = TRUE}
data("RLdata500")
head(RLdata500)
```

We create a new column (`id_count`) that indicates how many times a
given unit occurs and then add leading zeros to the `bm` and `bd`
columns. Finally, we create a new string column that concatenates the
information from all columns (excluding `rec_id`, `ent_id` and
`id_count`) in each row.

```{r RLdata500_concat, echo = TRUE}
RLdata500[, id_count :=.N, ent_id]
RLdata500[, bm:=sprintf("%02d", bm)]
RLdata500[, bd:=sprintf("%02d", bd)]
RLdata500[, txt:=tolower(paste0(fname_c1,fname_c2,lname_c1,lname_c2,by,bm,bd))]
head(RLdata500)
```

As in the previous example, we use the `txt` column in the `blocking`
function. This time, we set `ann = hnsw` to use the Hierarchical
Navigable Small World (HNSW) algorithm from the \CRANpkg{RcppHNSW}
package and `graph = TRUE` to obtain an \CRANpkg{igraph} object for
visualization.

```{r dedup_hnsw, echo = TRUE}
result_dedup_hnsw <- blocking(x = RLdata500$txt,
                              ann = "hnsw",
                              graph = TRUE,
                              verbose = 1)
```

The results are as follows.

```{r dedup_hnsw_result, echo = TRUE}
result_dedup_hnsw
```

Now we visualize connections using the obtained graph.

```{r dedup-graph, echo = TRUE, out.width = "100%", fig.width = 3, fig.height = 3, layout = "l-body", fig.align = "center", fig.cap = "Connection graph", label = "connection-graph", fig.pos = "H"}
plot(result_dedup_hnsw$graph, vertex.size = 1, vertex.label = NA)
```

We create a long `data.table` with information on blocks and units from
the original dataset.

```{r dedup_melted, echo = TRUE}
df_block_melted <- melt(result_dedup_hnsw$result, id.vars = c("block", "dist"))
df_block_melted_rec_block <- unique(df_block_melted[, .(rec_id=value, block)])
head(df_block_melted_rec_block)
```

We add the block information to the final dataset.

```{r dedup_blocks, echo = TRUE}
RLdata500[df_block_melted_rec_block, on = "rec_id", block_id := i.block]
head(RLdata500)
```

We can check in how many blocks the same entities (`ent_id`) are
observed. In our example, all the same entities are in the same blocks.

```{r dedup_uniq_blocs, echo = TRUE}
RLdata500[, .(uniq_blocks = uniqueN(block_id)), .(ent_id)][, .N, uniq_blocks]
```

Now we can visualize the distances between the units stored in the
\linebreak `result_dedup_hnsw$result` dataset. Clearly we have a mixture
of two groups: matches (close to 0) and non-matches (close to 1).

```{r dedup-hist, echo = TRUE, out.width = "100%", fig.width = 5, fig.height = 3, layout = "l-body", fig.align = "center", fig.cap = "Distances calculated between units", label = "dedup-hist", fig.pos = "H"}
hist(result_dedup_hnsw$result$dist, xlab = "Distances",
     ylab = "Frequency", breaks = "fd",
     main = "Distances calculated between units")
```

Finally, we visualize the result based on the information whether a
block contains matches or not.

```{r dedup-density, echo = TRUE, out.width = "100%", fig.width = 6, fig.height=5, layout="l-body", fig.align = 'center', fig.cap = "Distribution of distances between clusters type", label = "dedup-density", fig.pos = "H"}
df_for_density <- copy(df_block_melted[block %in% RLdata500$block_id])
df_for_density[, match:= block %in% RLdata500[id_count == 2]$block_id]

plot(density(df_for_density[match==FALSE]$dist),
     col = "blue", xlim = c(0, 0.8), 
     main = "Distribution of distances between\n
     clusters type (match=red, non-match=blue)")
lines(density(df_for_density[match==TRUE]$dist),
      col = "red", xlim = c(0, 0.8))
```

Now we compare the evaluation metrics across all ANN algorithms
supported by the `blocking` function, i.e. NND, HNSW, Approximate
Nearest Neighbors Oh Yeah (Annoy, from the \CRANpkg{RcppAnnoy} package),
Locality-sensitive hashing (LSH, from the \CRANpkg{mlpack} package), and
k-Nearest Neighbors (kNN -- denoted as `"kd"`, from the \CRANpkg{mlpack}
package). We use the `rec_id` and `ent_id` columns from the `RLdata500`
dataset to specify the true blocks and then calculate evaluation metrics
for all algorithms. Additionally, we assess blocking using the `klsh()`
function from the \CRANpkg{klsh} package, configured to create 10 blocks
and 100 blocks, respectively. In both settings, we use 20 random
projections and 2-character shingles. The results are as follows
(`klsh_10` and `klsh_100` refer to the `klsh` algorithm with 10 blocks
and 100 blocks, respectively).

```{r comparision, echo = TRUE}
set.seed(2025)
true_blocks <- RLdata500[, c("rec_id", "ent_id"), with = FALSE]
setnames(true_blocks, old = c("rec_id", "ent_id"), c("x", "block"))
eval_metrics <- list()
ann <- c("nnd", "hnsw", "annoy", "lsh","kd")
for (algorithm in ann) {
  eval_metrics[[algorithm]] <- blocking(x = RLdata500$txt,
                                ann = algorithm,
                                true_blocks = true_blocks)$metrics
}

blocks_klsh_10 <- klsh::klsh(
  r.set = RLdata500[, c("fname_c1", "fname_c2", "lname_c1",
                        "lname_c2", "by", "bm", "bd")],
  p = 20,
  num.blocks = 10,
  k = 2)
klsh_10_metrics <- klsh::confusion.from.blocking(
  blocking = blocks_klsh_10, 
  true_ids = RLdata500$ent_id)[-1]
klsh_10_metrics$f1_score <- 2 * klsh_10_metrics$precision *
  klsh_10_metrics$recall / 
  (klsh_10_metrics$precision + klsh_10_metrics$recall)
eval_metrics$klsh_10 <- unlist(klsh_10_metrics)
blocks_klsh_100 <- klsh::klsh(
  r.set = RLdata500[, c("fname_c1", "fname_c2", "lname_c1",
                        "lname_c2", "by", "bm", "bd")],
  p = 20,
  num.blocks = 100,
  k = 2)
klsh_100_metrics <- klsh::confusion.from.blocking(
  blocking = blocks_klsh_100, 
  true_ids = RLdata500$ent_id)[-1]
klsh_100_metrics$f1_score <- 2 * klsh_100_metrics$precision * 
  klsh_100_metrics$recall /
  (klsh_100_metrics$precision + klsh_100_metrics$recall)
eval_metrics$klsh_100 <- unlist(klsh_100_metrics)

do.call(rbind, eval_metrics) * 100
```

# Summary

In this paper we have demonstrated the basic use cases of the
\CRANpkg{blocking} package. We believe that the software will be useful
for researchers working in various fields where integration of multiple
sources is an important aspect.

# Acknowledgements

Work on this package is supported by the National Science Centre, OPUS
20 grant no. 2020/39/B/HS4/00941. We also thank participants of the uRos
2024 conference for valuable comments and discussion.

We also have developed a python version of the package {BlockingPy} that
is available through the PiPy. It has the similar structure but offers
more ANN algorithms (e.g. FAISS) or usage of embeddings. For more
details see: Strojny, T., & Beręsewicz, M. (2025). BlockingPy:
approximate nearest neighbours for blocking of records for entity
resolution. arXiv preprint arXiv:2504.04266.
