% !TeX root = RJwrapper.tex
\title{Blocking: An R Package for Blocking of Records for Record Linkage and Deduplication}


\author{by Maciej BerÄ™sewicz and Adam Struzik}

\maketitle

\abstract{%
Entity resolution (probabilistic record linkage, deduplication) is essential for estimation based on multiple sources. It aims to link records without common identifiers that refer to the same entity (e.g., person, company). Without identifiers, researchers must specify which records to compare to calculate matching probability and reduce computational complexity. Traditional deterministic blocking uses common variables like names or dates of birth, but assumes error-free, complete data. To address this limitation, we developed the R package \CRANpkg{blocking}, which uses approximate nearest neighbour search and graph algorithms to reduce number of comparisons. This paper presents the package design, functionalities, and two case studies.
}

\section{Introduction}\label{introduction}

\subsection{Blocking for record linkage}\label{blocking-for-record-linkage}

Entity resolution (probabilistic record linkage, deduplication) is essential for estimation based on multiple sources (for recent review see \citet{Binette2022}). The goal is to link records without common identifiers that refer to the same entity (e.g., person, company). This situation is often observed in administrative records, in particular for foreign-born populations. For instance, the Social Insurance Institution register in Poland at the end of 2023 included 1.206 million records which referred to possibly 1.105 million individuals, out of which about 10\% had missing information in the personal identifier (PESEL) and about 50\% of cases had missing address details. Please note that the exact number of individuals will be certainly lower than 1.105 million as the 10\% may include duplicates.

This drives a need to link records without identifiers but often requires certain assumptions such as how to reduce the large number of possible comparisons as it is not possible to compare all pairs of records in a large dataset (e.g., for the mentioned example this would lead to over 600 billion comparisons). That is why \emph{blocking} methods are applied to reduce the number of comparisons prior to the final record linkage/deduplication stage not only because of computational reasons but also due to clerical review workload.

Blocking is a method of reducing the number of possible comparisons by assuming that certain variables should be exactly matched. For instance, a standard method is based on assuming that sex or age (or some other combination) should match exactly while other characteristics of the records could be varying. Another standard method is to use phonetic algorithms such as SOUNDEX or its improvements for non-English languages. Furthermore, due to the use of large language models one may also consider using embeddings to search for the closest neighbor and treat this as a possible pair. For a review of blocking methods see \citet{Steorts2014} or \citet{Papadakis2020} and in Section \ref{sec-software} we will discuss R packages that implement blocking methods.

Reducing the number of pairs has its costs: missing comparisons which lead to an increased false positive rate (FPR) and false negative rate (FNR) of the linkage study. In order to assess this error, a subset of pairs or simulation studies should be applied. Alternatively, one may consider approaches proposed by \citet{dasylva2021estimating} and \citet{dasylva2022consistent} who proposed methods to estimate FPR and FNR without access to the audit sample.

\subsection{Existing software and our contribution}\label{sec-software}

The R system offers several packages that implements various blocking techniques which we grouped by the following classification:

\begin{itemize}
\tightlist
\item
  deterministic grouping:

  \begin{itemize}
  \tightlist
  \item
    \CRANpkg{reclin2} \citep[\citet{reclin2-rjournal}]{reclin2} which allows to pair records using the \texttt{pair\_blocking()} with a prespecified list of columns in a \texttt{data.frame}, and the \texttt{pair\_minsim()} function that allows to specify the minimal similarity score (e.g.~1 out of 3 variables should match exactly).
  \item
    \CRANpkg{RecordLinkage} \citep[\citet{RecordLinkage-rjournal}]{RecordLinkage} which allows to specify blocking variable in the \texttt{blockfld} in either in \texttt{compare.dedup()} or \texttt{compare.linkage()} functions in a form of a vector (either character or numeric).
  \item
    \CRANpkg{fastLink} \citep[\citet{enamorado2019using}]{fastLink} which implements various blocking methods via the \texttt{blockData()} function such as exact matching, window matching (e.g., no more than 2 years difference between birth year) or k-means clustering algorithm. It should be noted that the \texttt{fastLink} returns splits dataset(s) into a separate lists while \texttt{reclin2} and \texttt{RecordLinkage} package create a single dataset.
  \end{itemize}
\item
  phonetic grouping:

  \begin{itemize}
  \tightlist
  \item
    \CRANpkg{RecordLinkage} allows to directly specify the phonetic comparison via the \texttt{phonetic} argument of the \texttt{compare.dedup()} or \texttt{compare.linkage()} function via the \texttt{soundex()} function. However, this is not used for blocking but for comparison of strings
  \item
    It should be noted that \CRANpkg{stringdist} \citep{stringdist} also implements SOUNDEX algorithm while the \CRANpkg{phonics} \citep[\citet{Phonetic2020}]{phonics} implements various phonetic algorithms that could be applied prior the blocking procedure (e.g., create a new column).
  \end{itemize}
\item
  probabilistic blocking:

  \begin{itemize}
  \tightlist
  \item
    \CRANpkg{klsh} \citep{klsh} is the only R package that implements probabilistic blocking using the k-means variant of locality sensitive hashing. The main \texttt{klsh()} function implements this approach and a resulting object is a list with row identifiers along for the pre specified number of blocks (via the \texttt{num.blocks} argument of the \texttt{klsh()} function).
  \end{itemize}
\end{itemize}

\subsection{Outline of article}\label{outline-of-article}

The paper has the following structure. In the section \ref{sec-blocks} we provide description of the main functionalities of the \texttt{blocking} package and how we can assess the result. In the section \ref{sec-case} we provide two case studies: probabilistic record linkage and deduplication. These examples show how our package can improve pipeline of entity resolution and work with existing R packages.

\section{\texorpdfstring{Blocking of records using \texttt{blocking} function}{Blocking of records using blocking function}}\label{sec-blocks}

\subsection{The main function}\label{the-main-function}

\subsection{Assessment of results}\label{assessment-of-results}

In the package we have implemented several measures that can be used to
assess the results

\textbf{Reduction Ratio}: Provides necessary details about the reduction in
comparison pairs if the given blocks are applied to a further record
linkage or deduplication procedure. For deduplication:

\[
\text{RR}_{\text{deduplication}} = 1 - \frac{\sum\limits_{i=1}^{k} \binom{|B_i|}{2}}{\binom{n}{2}},
\]

where \(k\) is the total number of blocks, \(n\) is the total number of
records in the dataset, and \(|B_i|\) is the number of records in the
\(i\)-th block. \(\sum\limits_{i=1}^{k} \binom{|B_i|}{2}\) is the number of
comparisons after blocking, while \(\binom{n}{2}\) is the total number of
possible comparisons without blocking. For record linkage the reduction
ratio is defined as follows

\[
\text{RR}_{\text{record\_linkage}} = 1 - \frac{\sum\limits_{i=1}^{k} |B_{i,x}| \cdot |B_{i,y}|} {(m \cdot n)},
\]

where \(m\) and \(n\) are the sizes of datasets \(X\) and \(Y\), and \(k\) is the
total number of blocks. The term \(|B_{i,x}|\) is the number of unique
records from dataset \(X\) in the \(i\)-th block, while \(|B_{i,y}|\) is the
number of unique records from dataset \(Y\) in the \(i\)-th block. The
expression \(\sum\limits_{i=1}^{k} |B_{i,x}| \cdot |B_{i,y}|\) is the
number of comparisons after blocking.

Confusion matrix presents results in comparison to ground-truth
\texttt{blocks} in a pairwise manner (e.g., one true positive pair
occurs when both records from the comparison pair belong to the same
predicted \texttt{block} and to the same ground-truth \texttt{block} in
the evaluation data frame).

\begin{itemize}
\tightlist
\item
  True Positive (TP): Record pairs correctly matched in the same
  block.
\item
  False Positive (FP): Records pairs identified as matches that are
  not true matches in the same block.
\item
  True Negative (TN): Record pairs correctly identified as non-matches
  (different blocks)
\item
  False Negative (FN): Records identified as non-matches that are true
  matches in the same block.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Formula}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Formula}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Recall & \(\frac{TP}{TP + FN}\) & Accuracy & \(\frac{TP + TN}{TP + TN + FP + FN}\) \\
Precision & \(\frac{TP}{TP + FP}\) & Specificity & \(\frac{TN}{TN + FP}\) \\
F1 Score & \(2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\) & False Positive Rate & \(\frac{FP}{FP + TN}\) \\
False Negative Rate & \(\frac{FN}{FN + TP}\) & & \\
\end{longtable}

\emph{Table: Evaluation Metrics}

\section{Case studies}\label{sec-case}

\subsection{Record linkage example}\label{record-linkage-example}

Let us first load the required packages.

\begin{verbatim}
library(blocking)
library(data.table)
\end{verbatim}

We demonstrate the use of \texttt{blocking} function for record linkage on the
\texttt{foreigners} dataset included in the package. This fictional
representation of the foreign population in Poland was generated based
on publicly available information, preserving the distributions from
administrative registers. It contains 110,000 rows with 100,000
entities. Each row represents one record, with the following columns:

\begin{itemize}
\tightlist
\item
  \texttt{fname} -- first name,
\item
  \texttt{sname} -- second name,
\item
  \texttt{surname} -- surname,
\item
  \texttt{date} -- date of birth,
\item
  \texttt{region} -- region (county),
\item
  \texttt{country} -- country,
\item
  \texttt{true\_id} -- person ID.
\end{itemize}

\begin{verbatim}
data(foreigners)
head(foreigners)
\end{verbatim}

\begin{verbatim}
#>     fname  sname    surname       date region country true_id
#>    <char> <char>     <char>     <char> <char>  <char>   <num>
#> 1:   emin            imanov 1998/02/05            031       0
#> 2: nurlan        suleymanli 2000/08/01            031       1
#> 3:   amio        maharrsmov 1939/03/08            031       2
#> 4:   amik        maharramof 1939/03/08            031       2
#> 5:   amil        maharramov 1993/03/08            031       2
#> 6:  gadir        jahangirov 1991/08/29            031       3
\end{verbatim}

We split the dataset into two separate files: one containing the first
appearance of each entity in the \texttt{foreigners} dataset, and the other
containing its subsequent appearances.

\begin{verbatim}
foreigners_1 <- foreigners[!duplicated(foreigners$true_id), ]
foreigners_2 <- foreigners[duplicated(foreigners$true_id), ]
\end{verbatim}

Now in both datasets we remove slashes from the \texttt{date} column and create
a new string column that concatenates the information from all columns
(excluding \texttt{true\_id}) in each row.

\begin{verbatim}
foreigners_1[, date := gsub("/", "", date)]
foreigners_1[, txt := paste0(fname, sname, surname, date, region, country)]
foreigners_2[, date := gsub("/", "", date)]
foreigners_2[, txt := paste0(fname, sname, surname, date, region, country)]
head(foreigners_1)
\end{verbatim}

\begin{verbatim}
#>     fname  sname    surname     date region country true_id
#>    <char> <char>     <char>   <char> <char>  <char>   <num>
#> 1:   emin            imanov 19980205            031       0
#> 2: nurlan        suleymanli 20000801            031       1
#> 3:   amio        maharrsmov 19390308            031       2
#> 4:  gadir        jahangirov 19910829            031       3
#> 5:   zaur         bayramova 19961006  01261     031       4
#> 6:   asif          mammadov 19970726            031       5
#>                              txt
#>                           <char>
#> 1:         eminimanov19980205031
#> 2:   nurlansuleymanli20000801031
#> 3:     amiomaharrsmov19390308031
#> 4:    gadirjahangirov19910829031
#> 5: zaurbayramova1996100601261031
#> 6:       asifmammadov19970726031
\end{verbatim}

\subsubsection{General use}\label{general-use}

We use the newly created columns in the \texttt{blocking} function, which
relies on the default \CRANpkg{rnndescent} (Nearest Neighbor Descent)
algorithm based on cosine distance. Additionally, we set \texttt{verbose\ =\ 1}
to monitor progress. Note that a default parameter of the \texttt{blocking}
function is \texttt{seed\ =\ 2023}, which sets the random seed.

\begin{verbatim}
result_reclin <- blocking(x = foreigners_1$txt,
                          y = foreigners_2$txt,
                          verbose = 1)
\end{verbatim}

\begin{verbatim}
#> ===== creating tokens =====
#> ===== starting search (nnd, x, y: 100000, 10000, t: 1232) =====
#> ===== creating graph =====
\end{verbatim}

Now we examine the results of record linkage.

\begin{itemize}
\tightlist
\item
  We have created
  6,470
  blocks.
\item
  The blocking process utilized
  1,232 columns (2
  character shingles).
\item
  We have 3,920 blocks of
  2 elements,
  1,599 blocks of
  3 elements,\ldots,
  2 blocks of
  7 elements.
\end{itemize}

\begin{verbatim}
result_reclin
\end{verbatim}

\begin{verbatim}
#> ========================================================
#> Blocking based on the nnd method.
#> Number of blocks: 6470.
#> Number of columns used for blocking: 1232.
#> Reduction ratio: 0.9999.
#> ========================================================
#> Distribution of the size of the blocks:
#>    2    3    4    5    6    7 
#> 3920 1599  928   19    2    2
\end{verbatim}

Structure of the object is as follows:

\begin{itemize}
\tightlist
\item
  \texttt{result} -- a \texttt{data.table} with identifiers and block IDs,
\item
  \texttt{method} -- name of the ANN algorithm used,
\item
  \texttt{deduplication} -- whether deduplication was applied,
\item
  \texttt{representation} -- whether shingles or vectors were used,
\item
  \texttt{metrics} -- metrics for quality assessment (here \texttt{NULL}),
\item
  \texttt{confusion} -- confusion matrix (here \texttt{NULL}),
\item
  \texttt{colnames} -- column names used for the comparison,
\item
  \texttt{graph} -- an \CRANpkg{igraph} object, mainly for visualization
  (here \texttt{NULL}).
\end{itemize}

\begin{verbatim}
str(result_reclin, 1)
\end{verbatim}

\begin{verbatim}
#> List of 8
#>  $ result        :Classes 'data.table' and 'data.frame': 10000 obs. of  4 variables:
#>   ..- attr(*, ".internal.selfref")=<externalptr> 
#>  $ method        : chr "nnd"
#>  $ deduplication : logi FALSE
#>  $ representation: chr "shingles"
#>  $ metrics       : NULL
#>  $ confusion     : NULL
#>  $ colnames      : chr [1:1232] "0a" "0b" "0c" "0m" ...
#>  $ graph         : NULL
#>  - attr(*, "class")= chr "blocking"
\end{verbatim}

The resulting \texttt{data.table} has four columns:

\begin{itemize}
\tightlist
\item
  \texttt{x} -- reference dataset (i.e.~\texttt{foreigners\_1}) -- this may not
  contain all units of \texttt{foreigners\_1},
\item
  \texttt{y} -- query (each row of \texttt{foreigners\_2}) -- this may not contain
  all units of \texttt{foreigners\_2},
\item
  \texttt{block} -- block ID,
\item
  \texttt{dist} -- distance between objects.
\end{itemize}

\begin{verbatim}
head(result_reclin$result)
\end{verbatim}

\begin{verbatim}
#>        x     y block      dist
#>    <int> <int> <num>     <num>
#> 1:     3     1     1 0.2216882
#> 2:     3     2     1 0.2122737
#> 3:    21     3     2 0.1172652
#> 4:    57     4     3 0.1863238
#> 5:    57     5     3 0.1379310
#> 6:    61     6     4 0.2307692
\end{verbatim}

Let's examine the first pair. Obviously, there are typos in the \texttt{fname}
and \texttt{surname}. Nevertheless, the pair is a match.

\begin{verbatim}
cbind(t(foreigners_1[3, 1:6]), t(foreigners_2[1, 1:6]))
\end{verbatim}

\begin{verbatim}
#>         [,1]         [,2]        
#> fname   "amio"       "amik"      
#> sname   ""           ""          
#> surname "maharrsmov" "maharramof"
#> date    "19390308"   "19390308"  
#> region  ""           ""          
#> country "031"        "031"
\end{verbatim}

Now we use the \texttt{true\_id} values to evaluate our approach.

\begin{verbatim}
matches <- merge(x = foreigners_1[, .(x = 1:.N, true_id)],
                 y = foreigners_2[, .(y = 1:.N, true_id)],
                 by = "true_id")
matches[, block := rleid(x)]
head(matches)
\end{verbatim}

\begin{verbatim}
#> Key: <true_id>
#>    true_id     x     y block
#>      <num> <int> <int> <int>
#> 1:       2     3     1     1
#> 2:       2     3     2     1
#> 3:      20    21     3     2
#> 4:      56    57     4     3
#> 5:      56    57     5     3
#> 6:      60    61     6     4
\end{verbatim}

We have 10,000 matched pairs. We use the \texttt{true\_blocks} parameter in the
\texttt{blocking} function to specify the true block assignments. We obtain the
quality metrics for the assessment of record linkage.

\begin{verbatim}
result_2_reclin <- blocking(x = foreigners_1$txt,
                            y = foreigners_2$txt,
                            verbose = 1,
                            true_blocks = matches[, .(x, y, block)])
\end{verbatim}

\begin{verbatim}
#> ===== creating tokens =====
#> ===== starting search (nnd, x, y: 100000, 10000, t: 1232) =====
#> ===== creating graph =====
\end{verbatim}

\begin{verbatim}
result_2_reclin
\end{verbatim}

\begin{verbatim}
#> ========================================================
#> Blocking based on the nnd method.
#> Number of blocks: 6470.
#> Number of columns used for blocking: 1232.
#> Reduction ratio: 0.9999.
#> ========================================================
#> Distribution of the size of the blocks:
#>    2    3    4    5    6    7 
#> 3920 1599  928   19    2    2 
#> ========================================================
#> Evaluation metrics (standard):
#>      recall   precision         fpr         fnr    accuracy specificity 
#>     96.7532     78.6700      0.0038      3.2468     99.9957     99.9962 
#>    f1_score 
#>     86.7795
\end{verbatim}

For example, our approach results in a
3.25\% false negative
rate (FNR). To improve this, we can increase the \texttt{epsilon} parameter of
the NND method from 0.1 to 0.5. To do so, we configure the \texttt{control\_ann}
parameter in the \texttt{blocking} function using the \texttt{controls\_ann} and
\texttt{control\_nnd} functions.

\begin{verbatim}
result_3_reclin <- blocking(x = foreigners_1$txt,
                            y = foreigners_2$txt,
                            verbose = 1,
                            true_blocks = matches[, .(x, y, block)],
                            control_ann = controls_ann(nnd = control_nnd(epsilon = 0.5)))
\end{verbatim}

\begin{verbatim}
#> ===== creating tokens =====
#> ===== starting search (nnd, x, y: 100000, 10000, t: 1232) =====
#> ===== creating graph =====
\end{verbatim}

\begin{verbatim}
result_3_reclin
\end{verbatim}

\begin{verbatim}
#> ========================================================
#> Blocking based on the nnd method.
#> Number of blocks: 6394.
#> Number of columns used for blocking: 1232.
#> Reduction ratio: 0.9999.
#> ========================================================
#> Distribution of the size of the blocks:
#>    2    3    4    5    7 
#> 3800 1615  954   21    4 
#> ========================================================
#> Evaluation metrics (standard):
#>      recall   precision         fpr         fnr    accuracy specificity 
#>     96.8776     80.0500      0.0036      3.1224     99.9960     99.9964 
#>    f1_score 
#>     87.6636
\end{verbatim}

That decreases the FNR to
3.12\%.

\subsection{Deduplication example}\label{deduplication-example}

We demonstrate deduplication using the \texttt{blocking} function on the
\texttt{RLdata500} dataset from the \CRANpkg{RecordLinkage} package. Note that
the dataset is included in the \texttt{blocking} package. It contains
artificial personal data. Fifty records have been duplicated with
randomly generated errors. Each row represents one record, with the
following columns:

\begin{itemize}
\tightlist
\item
  \texttt{fname\_c1} -- first name, first component,
\item
  \texttt{fname\_c2} -- first name, second component,
\item
  \texttt{lname\_c1} -- last name, first component,
\item
  \texttt{lname\_c2} -- last name, second component,
\item
  \texttt{by} -- year of birth,
\item
  \texttt{bm} -- month of birth,
\item
  \texttt{bd} -- day of birth,
\item
  \texttt{rec\_id} -- record id,
\item
  \texttt{ent\_id} -- entity id.
\end{itemize}

\begin{verbatim}
data(RLdata500)
head(RLdata500)
\end{verbatim}

\begin{verbatim}
#>    fname_c1 fname_c2 lname_c1 lname_c2    by    bm    bd rec_id ent_id
#>      <char>   <char>   <char>   <char> <int> <int> <int>  <int>  <int>
#> 1:  CARSTEN             MEIER           1949     7    22      1     34
#> 2:     GERD             BAUER           1968     7    27      2     51
#> 3:   ROBERT          HARTMANN           1930     4    30      3    115
#> 4:   STEFAN             WOLFF           1957     9     2      4    189
#> 5:     RALF           KRUEGER           1966     1    13      5     72
#> 6:  JUERGEN            FRANKE           1929     7     4      6    142
\end{verbatim}

We create a new column (\texttt{id\_count}) that indicates how many times a
given unit occurs and then add leading zeros to the \texttt{bm} and \texttt{bd}
columns. Finally, we create a new string column that concatenates the
information from all columns (excluding \texttt{rec\_id}, \texttt{ent\_id} and
\texttt{id\_count}) in each row.

\begin{verbatim}
RLdata500[, id_count :=.N, ent_id]
RLdata500[, bm:=sprintf("%02d", bm)]
RLdata500[, bd:=sprintf("%02d", bd)]
RLdata500[, txt:=tolower(
  paste0(fname_c1,fname_c2,lname_c1,lname_c2,by,bm,bd))]
head(RLdata500)
\end{verbatim}

\begin{verbatim}
#>    fname_c1 fname_c2 lname_c1 lname_c2    by     bm     bd rec_id ent_id
#>      <char>   <char>   <char>   <char> <int> <char> <char>  <int>  <int>
#> 1:  CARSTEN             MEIER           1949     07     22      1     34
#> 2:     GERD             BAUER           1968     07     27      2     51
#> 3:   ROBERT          HARTMANN           1930     04     30      3    115
#> 4:   STEFAN             WOLFF           1957     09     02      4    189
#> 5:     RALF           KRUEGER           1966     01     13      5     72
#> 6:  JUERGEN            FRANKE           1929     07     04      6    142
#>    id_count                    txt
#>       <int>                 <char>
#> 1:        1   carstenmeier19490722
#> 2:        2      gerdbauer19680727
#> 3:        1 roberthartmann19300430
#> 4:        1    stefanwolff19570902
#> 5:        1    ralfkrueger19660113
#> 6:        1  juergenfranke19290704
\end{verbatim}

As in the previous example, we use the \texttt{txt} column in the \texttt{blocking}
function. This time, we set \texttt{ann\ =\ hnsw} to use the Hierarchical
Navigable Small World (HNSW) algorithm from the \CRANpkg{RcppHNSW}
package and \texttt{graph\ =\ TRUE} to obtain an \CRANpkg{igraph} object for
visualization.

\begin{verbatim}
result_dedup_hnsw <- blocking(x = RLdata500$txt,
                              ann = "hnsw",
                              graph = TRUE,
                              verbose = 1)
\end{verbatim}

\begin{verbatim}
#> ===== creating tokens =====
#> ===== starting search (hnsw, x, y: 500, 500, t: 429) =====
#> ===== creating graph =====
\end{verbatim}

The results are as follows.

\begin{verbatim}
result_dedup_hnsw
\end{verbatim}

\begin{verbatim}
#> ========================================================
#> Blocking based on the hnsw method.
#> Number of blocks: 133.
#> Number of columns used for blocking: 429.
#> Reduction ratio: 0.9916.
#> ========================================================
#> Distribution of the size of the blocks:
#>  2  3  4  5  6  7  8  9 10 11 12 17 
#> 46 35 23  8  6  6  2  3  1  1  1  1
\end{verbatim}

\begin{verbatim}
head(result_dedup_hnsw$result)
\end{verbatim}

\begin{verbatim}
#>        x     y block       dist
#>    <int> <int> <num>      <num>
#> 1:     1    64    35 0.47379863
#> 2:     2    43     1 0.08074522
#> 3:     2   486     1 0.41023219
#> 4:     3   450    88 0.43263358
#> 5:     4    50    13 0.52565831
#> 6:     5   128     2 0.51333570
\end{verbatim}

Now we visualize connections using the obtained graph.

\begin{verbatim}
plot(result_dedup_hnsw$graph, vertex.size = 1, vertex.label = NA)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{paper-blocking_files/figure-latex/connection-graph-1} 

}

\caption{Connection graph}\label{fig:connection-graph}
\end{figure}

We create a long \texttt{data.table} with information on blocks and units from
the original dataset.

\begin{verbatim}
df_block_melted <- melt(result_dedup_hnsw$result, id.vars = c("block", "dist"))
df_block_melted_rec_block <- unique(df_block_melted[, .(rec_id=value, block)])
head(df_block_melted_rec_block)
\end{verbatim}

\begin{verbatim}
#>    rec_id block
#>     <int> <num>
#> 1:      1    35
#> 2:      2     1
#> 3:      3    88
#> 4:      4    13
#> 5:      5     2
#> 6:      6    35
\end{verbatim}

We add the block information to the final dataset.

\begin{verbatim}
RLdata500[df_block_melted_rec_block, on = "rec_id", block_id := i.block]
head(RLdata500)
\end{verbatim}

\begin{verbatim}
#>    fname_c1 fname_c2 lname_c1 lname_c2    by     bm     bd rec_id ent_id
#>      <char>   <char>   <char>   <char> <int> <char> <char>  <int>  <int>
#> 1:  CARSTEN             MEIER           1949     07     22      1     34
#> 2:     GERD             BAUER           1968     07     27      2     51
#> 3:   ROBERT          HARTMANN           1930     04     30      3    115
#> 4:   STEFAN             WOLFF           1957     09     02      4    189
#> 5:     RALF           KRUEGER           1966     01     13      5     72
#> 6:  JUERGEN            FRANKE           1929     07     04      6    142
#>    id_count                    txt block_id
#>       <int>                 <char>    <num>
#> 1:        1   carstenmeier19490722       35
#> 2:        2      gerdbauer19680727        1
#> 3:        1 roberthartmann19300430       88
#> 4:        1    stefanwolff19570902       13
#> 5:        1    ralfkrueger19660113        2
#> 6:        1  juergenfranke19290704       35
\end{verbatim}

We can check in how many blocks the same entities (\texttt{ent\_id}) are
observed. In our example, all the same entities are in the same blocks.

\begin{verbatim}
RLdata500[, .(uniq_blocks = uniqueN(block_id)), .(ent_id)][, .N, uniq_blocks]
\end{verbatim}

\begin{verbatim}
#>    uniq_blocks     N
#>          <int> <int>
#> 1:           1   450
\end{verbatim}

Now we can visualize the distances between the units stored in the
\linebreak \texttt{result\_dedup\_hnsw\$result} dataset. Clearly we have a mixture
of two groups: matches (close to 0) and non-matches (close to 1).

\begin{verbatim}
hist(result_dedup_hnsw$result$dist, xlab = "Distances",
     ylab = "Frequency", breaks = "fd",
     main = "Distances calculated between units")
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{paper-blocking_files/figure-latex/dedup-hist-1} 

}

\caption{Distances calculated between units}\label{fig:dedup-hist}
\end{figure}

Finally, we visualize the result based on the information whether a
block contains matches or not.

\begin{verbatim}
df_for_density <- copy(df_block_melted[block %in% RLdata500$block_id])
df_for_density[, match:= block %in% RLdata500[id_count == 2]$block_id]

plot(density(df_for_density[match==FALSE]$dist),
     col = "blue", xlim = c(0, 0.8), 
     main = "Distribution of distances between\n
     clusters type (match=red, non-match=blue)")
lines(density(df_for_density[match==TRUE]$dist),
      col = "red", xlim = c(0, 0.8))
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{paper-blocking_files/figure-latex/dedup-density-1} 

}

\caption{Distribution of distances between clusters type}\label{fig:dedup-density}
\end{figure}

Now we compare the evaluation metrics across all ANN algorithms
supported by the \texttt{blocking} function, i.e.~NND, HNSW, Approximate
Nearest Neighbors Oh Yeah (Annoy, from the \CRANpkg{RcppAnnoy} package),
Locality-sensitive hashing (LSH, from the \CRANpkg{mlpack} package), and
k-Nearest Neighbors (kNN -- denoted as \texttt{"kd"}, from the \CRANpkg{mlpack}
package). We use the \texttt{rec\_id} and \texttt{ent\_id} columns from the \texttt{RLdata500}
dataset to specify the true blocks and then calculate evaluation metrics
for all algorithms. Additionally, we assess blocking using the \texttt{klsh}
function from the \CRANpkg{klsh} package, configured to create 10 blocks
and 100 blocks, respectively. In both settings, we use 20 random
projections and 2-character shingles. The results are as follows
(\texttt{klsh\_10} and \texttt{klsh\_100} refer to the \texttt{klsh} algorithm with 10 blocks
and 100 blocks, respectively).

\begin{verbatim}
true_blocks <- RLdata500[, c("rec_id", "ent_id"), with = FALSE]
setnames(true_blocks, old = c("rec_id", "ent_id"), c("x", "block"))
eval_metrics <- list()
ann <- c("nnd", "hnsw", "annoy", "lsh","kd")
for (algorithm in ann) {
  eval_metrics[[algorithm]] <- blocking(x = RLdata500$txt,
                                ann = algorithm,
                                true_blocks = true_blocks)$metrics
}

set.seed(2025)
blocks_klsh_10 <- klsh::klsh(
  r.set = RLdata500[, c("fname_c1", "fname_c2", "lname_c1",
                        "lname_c2", "by", "bm", "bd")],
  p = 20,
  num.blocks = 10,
  k = 2)
klsh_10_metrics <- klsh::confusion.from.blocking(
  blocking = blocks_klsh_10, 
  true_ids = RLdata500$ent_id)[-1]
klsh_10_metrics$f1_score <- 2 * klsh_10_metrics$precision *
  klsh_10_metrics$recall / 
  (klsh_10_metrics$precision + klsh_10_metrics$recall)
eval_metrics$klsh_10 <- unlist(klsh_10_metrics)
blocks_klsh_100 <- klsh::klsh(
  r.set = RLdata500[, c("fname_c1", "fname_c2", "lname_c1",
                        "lname_c2", "by", "bm", "bd")],
  p = 20,
  num.blocks = 100,
  k = 2)
klsh_100_metrics <- klsh::confusion.from.blocking(
  blocking = blocks_klsh_100, 
  true_ids = RLdata500$ent_id)[-1]
klsh_100_metrics$f1_score <- 2 * klsh_100_metrics$precision * 
  klsh_100_metrics$recall /
  (klsh_100_metrics$precision + klsh_100_metrics$recall)
eval_metrics$klsh_100 <- unlist(klsh_100_metrics)

do.call(rbind, eval_metrics) * 100
\end{verbatim}

\begin{verbatim}
#>          recall precision       fpr fnr accuracy specificity f1_score
#> nnd         100 5.1706308 0.7353649   0 99.26493    99.26464 9.832842
#> hnsw        100 4.7573739 0.8027265   0 99.19760    99.19727 9.082652
#> annoy       100 4.8030740 0.7947073   0 99.20561    99.20529 9.165903
#> lsh          98 1.0403397 3.7377706   2 96.26293    96.26223 2.058824
#> kd          100 5.1921080 0.7321572   0 99.26814    99.26784 9.871668
#> klsh_10      82 0.3290794 9.9582999  18 90.03848    90.04170 0.655528
#> klsh_100     86 3.4649476 0.9607057  14 99.03407    99.03929 6.661503
\end{verbatim}

\section{Summary}\label{summary}

In this paper we have demonstrated the basic use cases of the
\CRANpkg{blocking} package. We believe that the software will be useful
for researchers working in various fields where integration of multiple
sources is an important aspect.

\section{Acknowledgements}\label{acknowledgements}

Work on this package is supported by the National Science Centre, OPUS
20 grant no. 2020/39/B/HS4/00941. We also thank participants of the uRos
2024 conference for valuable comments and discussion.

We also have developed a python version of the package \{BlockingPy\} that
is available through the PiPy. It has the similar structure but offers
more ANN algorithms (e.g.~FAISS) or usage of embeddings. For more
details see: Strojny, T., \& BerÄ™sewicz, M. (2025). BlockingPy:
approximate nearest neighbours for blocking of records for entity
resolution. arXiv preprint arXiv:2504.04266.

\bibliography{RJreferences.bib}

\address{%
Maciej BerÄ™sewicz\\
University of Economics and BusinessStatisical Office in PoznaÅ„\\%
Department of Statistics, PoznaÅ„, Poland\\ Centre for the Methodology of Population Studies\\
%
\url{https://maciejberesewicz.com}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-8281-4301}{0000-0002-8281-4301}}\\%
\href{mailto:maciej.beresewicz@poznan.pl}{\nolinkurl{maciej.beresewicz@poznan.pl}}%
}

\address{%
Adam Struzik\\
Adam Mickiewicz UniversityStatisical Office in PoznaÅ„\\%
Department of Mathematics, PoznaÅ„, Poland\\ Centre for Urban Statistics\\
%
%
%
\href{mailto:adastr5@st.amu.edu.pl}{\nolinkurl{adastr5@st.amu.edu.pl}}%
}
